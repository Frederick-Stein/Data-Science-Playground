{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/5h59m0BYG9OOPKgFWIA7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Frederick-Stein/Data-Science-Playground/blob/main/MiniTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SN8Wyz1dKeG8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ## mini sentence encode\n",
        "# token_to_id = {\n",
        "#     \"what\": 0,\n",
        "#     \"is\": 1,\n",
        "#     \"the\": 2,\n",
        "#     \"weather\": 3,\n",
        "#     \"today\": 4,\n",
        "#     \"sunny\": 5,\n",
        "#     'rainy': 6,\n",
        "#     'tomorrow': 7,\n",
        "#     \"<EOS>\": 8,\n",
        "#     }\n",
        "\n",
        "# id_to_token = {v: k for k, v in token_to_id.items()}\n",
        "\n",
        "# # input_sentence = \"what is the weather today <EOS> sunny\"\n",
        "# # output_sentence = \"is the weather today <EOS> sunny <EOS>\"\n",
        "\n",
        "# # input_tokens = input_sentence.split()\n",
        "# # output_tokens = output_sentence.split()\n",
        "\n",
        "# # input_ids = torch.tensor([token_to_id[token] for token in input_tokens])\n",
        "# # output_ids = torch.tensor([token_to_id[token] for token in output_tokens])\n",
        "\n",
        "\n",
        "# input_ids = torch.tensor([[token_to_id[\"what\"], token_to_id[\"is\"], token_to_id[\"the\"], token_to_id[\"weather\"], token_to_id[\"today\"], token_to_id[\"<EOS>\"], token_to_id[\"sunny\"]],\n",
        "#                        [token_to_id[\"the\"], token_to_id['weather'], token_to_id['tomorrow'], token_to_id['is'], token_to_id['what'], token_to_id['<EOS>'], token_to_id['rainy']]\n",
        "#                        ])\n",
        "\n",
        "# output_ids = torch.tensor([\n",
        "#     [token_to_id['is'], token_to_id['the'], token_to_id[\"weather\"], token_to_id[\"today\"], token_to_id[\"<EOS>\"], token_to_id[\"sunny\"], token_to_id['<EOS>']],\n",
        "#     [token_to_id['weather'], token_to_id['tomorrow'], token_to_id['is'], token_to_id['what'], token_to_id['<EOS>'], token_to_id['rainy'], token_to_id['<EOS>']],\n",
        "# ])\n",
        "\n",
        "# train_data = TensorDataset(input_ids, output_ids)\n",
        "# train_loader = DataLoader(train_data, batch_size=2, shuffle=True)"
      ],
      "metadata": {
        "id": "73oo5OcVQjD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## generalized sentence encode\n",
        "sentence1 = \"what is the weather today sunny\"\n",
        "sentence2 = \"please tell me the weather tomorrow rainy\"\n",
        "sentences = [sentence1, sentence2]\n",
        "\n",
        "words = set()\n",
        "for sentence in sentences:\n",
        "    for word in sentence.split():\n",
        "        words.add(word)\n",
        "\n",
        "token_to_id = {\"<PAD>\": 0, \"<EOS>\": 1}\n",
        "id_to_token = {0: \"<PAD>\", 1: \"<EOS>\"}\n",
        "for i, word in enumerate(words, start = 2):\n",
        "    token_to_id[word] = i\n",
        "    id_to_token[i] = word\n",
        "print(token_to_id)\n",
        "print(id_to_token)\n",
        "\n",
        "def encode(sentence):\n",
        "    ids = []\n",
        "    for word in sentence.split():\n",
        "        ids.append(token_to_id[word])\n",
        "    return ids\n",
        "\n",
        "encoded_sentences = []\n",
        "for sentence in sentences:\n",
        "    encoded = encode(sentence)\n",
        "    encoded.append(token_to_id[\"<EOS>\"])\n",
        "    encoded_sentences.append(torch.tensor(encoded))\n",
        "\n",
        "input_ids = nn.utils.rnn.pad_sequence(encoded_sentences, batch_first=True, padding_value=0)\n",
        "output_ids = input_ids.clone()\n",
        "output_ids[:, :-1] = input_ids[:, 1:]\n",
        "output_ids[:, -1] = 0\n",
        "print(input_ids)\n",
        "print(output_ids)\n",
        "\n",
        "train_data = TensorDataset(input_ids, output_ids)\n",
        "train_loader = DataLoader(train_data, batch_size=2, shuffle=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-OqnYYFBjYR",
        "outputId": "a152d154-c32e-44f2-d2e2-49603bba9a36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<PAD>': 0, '<EOS>': 1, 'what': 2, 'tell': 3, 'weather': 4, 'please': 5, 'is': 6, 'rainy': 7, 'today': 8, 'me': 9, 'sunny': 10, 'tomorrow': 11, 'the': 12}\n",
            "{0: '<PAD>', 1: '<EOS>', 2: 'what', 3: 'tell', 4: 'weather', 5: 'please', 6: 'is', 7: 'rainy', 8: 'today', 9: 'me', 10: 'sunny', 11: 'tomorrow', 12: 'the'}\n",
            "tensor([[ 2,  6, 12,  4,  8, 10,  1,  0],\n",
            "        [ 5,  3,  9, 12,  4, 11,  7,  1]])\n",
            "tensor([[ 6, 12,  4,  8, 10,  1,  0,  0],\n",
            "        [ 3,  9, 12,  4, 11,  7,  1,  0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model = 2, vocab_size = 6):\n",
        "\n",
        "        ## d_model is the dim word embeddings\n",
        "        ## max_len is length of longest sentence we can generate\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        pe = torch.zeros(vocab_size , d_model)\n",
        "\n",
        "        pos = torch.arange(0, vocab_size , step = 1, dtype = torch.float).unsqueeze(1) # (max_len, 1)\n",
        "        embedding_idx = torch.arange(0, d_model, step = 2, dtype = torch.float) # (d/2,)\n",
        "\n",
        "        div_term = 1 / torch.pow(10000, (2 * embedding_idx) / d_model)\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(pos * div_term) # # (max_len, d/2)\n",
        "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0) # (1, max_len, d)\n",
        "\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, word_embeddings):\n",
        "        B, L, d = word_embeddings.shape # expects shape (B, L, d)\n",
        "        return word_embeddings + (self.pe[:, :L, :]).requires_grad_(False)"
      ],
      "metadata": {
        "id": "ntQWaxwbK2py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model = 2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model, bias = False)\n",
        "        self.W_k = nn.Linear(d_model, d_model, bias = False)\n",
        "        self.W_v = nn.Linear(d_model, d_model, bias = False)\n",
        "\n",
        "    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask = None):\n",
        "        # encodings_* shapes: (B, Lq, d), (B, Lk, d), (B, Lk, d)\n",
        "\n",
        "        Q = self.W_q(encodings_for_q) # (B, Lq, d)\n",
        "        K = self.W_k(encodings_for_k) # (B, Lk, d)\n",
        "        V = self.W_v(encodings_for_v) # (B, Lv, d)\n",
        "\n",
        "        ## softmax(Q * K^T / sqrt(d_k) + M) * V\n",
        "        similarity = Q @ K.transpose(-1, -2) # Q * K^T\n",
        "\n",
        "        # d_k\n",
        "        d_k = K.size(-1)\n",
        "\n",
        "        # scaled_simalrity = simalrity / torch.sqrt(torch.tensor(K.shape(-1))) # Q * K^T / sqrt(d_k)\n",
        "        scaled_similarity = similarity / math.sqrt(d_k)# Q * K^T / sqrt(d_k)\n",
        "\n",
        "        if mask is not None:\n",
        "            scaled_similarity = scaled_similarity.masked_fill(mask, float(\"-inf\")) # Q * K^T / sqrt(d_k) + M\n",
        "\n",
        "        attention_weights = F.softmax(scaled_similarity, dim = -1) # softmax(Q * K^T / sqrt(d_k) + M)\n",
        "\n",
        "        attention_scores = attention_weights @ V # softmax(Q * K^T / sqrt(d_k) + M) * V\n",
        "\n",
        "        return attention_scores"
      ],
      "metadata": {
        "id": "eeFJliC_T49_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MiniTransformer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model = 2, num_tokens = 7, vocab_size  = 7):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(num_embeddings = num_tokens, embedding_dim = d_model)\n",
        "\n",
        "        self.pe = PositionEncoding(d_model = d_model, vocab_size  = vocab_size )\n",
        "\n",
        "        self.attention = Attention(d_model = d_model)\n",
        "\n",
        "        self.fc_layer = nn.Linear(d_model, num_tokens)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # inputs: (B, L) of token ids\n",
        "        B, L = inputs.shape\n",
        "\n",
        "        word_embeddings = self.embedding(inputs)\n",
        "\n",
        "        pos_encoded = self.pe(word_embeddings)\n",
        "\n",
        "        device = inputs.device\n",
        "\n",
        "        mask = torch.triu(torch.ones(L, L, dtype=torch.bool, device=device), diagonal=1).unsqueeze(0)\n",
        "\n",
        "        self_attention_values = self.attention(pos_encoded, pos_encoded, pos_encoded, mask)\n",
        "\n",
        "        x = pos_encoded + self_attention_values # residual\n",
        "\n",
        "        outputs = self.fc_layer(x) # logits\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "ggt3vPIdfJdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MiniTransformer(num_tokens = len(token_to_id), d_model = 4, vocab_size  = 8)\n",
        "# input = torch.tensor([token_to_id[\"what\"], token_to_id[\"is\"], token_to_id[\"the\"], token_to_id[\"weather\"], token_to_id[\"today\"], token_to_id[\"<EOS>\"]])\n",
        "test = \"what is the weather today <EOS>\"\n",
        "input = torch.tensor(encode(test))\n",
        "\n",
        "input_length = input.size(0)\n",
        "predictions = model(input.unsqueeze(0)).squeeze(0)\n",
        "print(predictions)\n",
        "predicted_id = torch.tensor([torch.argmax(predictions[-1, :]).detach()])\n",
        "\n",
        "predicted_ids = predicted_id\n",
        "\n",
        "\n",
        "max_length = 8\n",
        "for i in range(input_length, max_length):\n",
        "    if (predicted_id == token_to_id[\"<EOS>\"]):\n",
        "        break\n",
        "    input = torch.cat((input, predicted_id))\n",
        "    predictions = model(input.unsqueeze(0)).squeeze(0)\n",
        "    predicted_id = torch.tensor([torch.argmax(predictions[-1, :]).detach()])\n",
        "    predicted_ids = torch.cat((predicted_ids, predicted_id))\n",
        "\n",
        "for id in predicted_ids:\n",
        "    print(id_to_token[id.item()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpnH9hLyvD4H",
        "outputId": "b5d8002c-c106-44ee-f21e-e8eb97076ace"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.1881, -1.4797,  0.5356, -0.9757,  0.1779, -0.1927,  0.5000, -0.3093,\n",
            "          0.8009,  0.7028, -0.6063,  0.4358,  0.2647],\n",
            "        [-1.7685, -1.4268, -0.8892, -1.0530,  0.5936,  0.5108,  0.2867, -1.6844,\n",
            "          1.0200,  0.9812,  0.9089,  1.2424, -0.3350],\n",
            "        [-0.7639, -0.5179, -0.0157, -0.3053,  0.2619, -0.3080,  0.7744, -0.3916,\n",
            "          0.6960,  1.1935, -0.1485,  0.9669,  0.5697],\n",
            "        [-0.1998,  0.6780,  0.7065, -0.4415,  1.5267,  0.0464,  1.1025, -0.0170,\n",
            "         -0.2636,  2.0673, -0.2618, -0.8357, -0.1464],\n",
            "        [ 0.6407, -0.3725,  1.3209, -0.1267, -0.1655, -0.9930,  0.7929,  0.7935,\n",
            "          0.3162,  0.4525, -1.4524, -0.0264,  0.9811],\n",
            "        [ 1.2281, -1.9536,  1.8152, -2.1869,  1.5132,  0.3883,  1.1817,  0.4988,\n",
            "          0.5537,  2.7715, -1.5823, -1.0980, -0.2590]],\n",
            "       grad_fn=<SqueezeBackward1>)\n",
            "me\n",
            "today\n",
            "tomorrow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## training process\n",
        "model = MiniTransformer(num_tokens = len(token_to_id), d_model = 16, vocab_size  = 8)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=token_to_id[\"<PAD>\"])\n",
        "optimizer = optim.Adam(model.parameters(), lr = 1e-2, weight_decay = 1e-3)\n",
        "\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    for X, y in train_loader:\n",
        "\n",
        "        y_pred = model(X)\n",
        "        B, L, d = y_pred.shape\n",
        "        loss = loss_fn(y_pred.view(B*L, d), y.view(B*L))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# X, y = next(iter(train_loader))\n",
        "# print(X.shape, y.shape)"
      ],
      "metadata": {
        "id": "Flx4cnZ049QF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for id in input_ids[0]:\n",
        "    print(id_to_token[id.item()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwKo5KSsDGzg",
        "outputId": "fc1106c2-2ba5-4875-a7f5-9ee76989faf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what\n",
            "is\n",
            "the\n",
            "weather\n",
            "today\n",
            "sunny\n",
            "<EOS>\n",
            "<PAD>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test1 = \"what is the weather today\"\n",
        "test2 = \"please tell me the weather tomorrow\"\n",
        "\n",
        "test = torch.tensor(encode(test1))\n",
        "test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84DALdBKOslB",
        "outputId": "1e47999d-7378-431b-dc92-a8063e5f3557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "test1 = \"what is the weather today\"\n",
        "test2 = \"please tell me the weather tomorrow\"\n",
        "test3 = \"what is the weather tomorrow\"\n",
        "test4 = \"weather today is\"\n",
        "\n",
        "test = torch.tensor(encode(test4))\n",
        "input_length = len(test)\n",
        "# input = torch.tensor([token_to_id[\"the\"], token_to_id[\"weather\"], token_to_id[\"is\"], token_to_id[\"what\"], token_to_id[\"tomorrow\"], token_to_id[\"<EOS>\"]])\n",
        "# input = torch.tensor([token_to_id[\"what\"], token_to_id[\"is\"], token_to_id[\"the\"], token_to_id[\"weather\"], token_to_id[\"tomorrow\"], token_to_id[\"<EOS>\"]])\n",
        "predictions = model(test.unsqueeze(0)).squeeze(0)\n",
        "predicted_id = torch.tensor([torch.argmax(predictions[-1, :]).detach()])\n",
        "predicted_ids = predicted_id\n",
        "\n",
        "max_length = 10\n",
        "for i in range(input_length, max_length):\n",
        "    if (predicted_id == token_to_id[\"<EOS>\"]):\n",
        "        break\n",
        "    test = torch.cat((test, predicted_id))\n",
        "    predictions = model(test.unsqueeze(0)).squeeze(0)\n",
        "    predicted_id = torch.tensor([torch.argmax(predictions[-1, :]).detach()])\n",
        "    predicted_ids = torch.cat((predicted_ids, predicted_id))\n",
        "\n",
        "for id in predicted_ids:\n",
        "    print(id_to_token[id.item()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtY5renAAmc8",
        "outputId": "9227afe1-8779-4de3-e924-bb5b12868bb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tomorrow\n",
            "rainy\n",
            "<EOS>\n"
          ]
        }
      ]
    }
  ]
}
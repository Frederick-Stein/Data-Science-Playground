{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNik24BXCVY9DtdL8QqS7Uh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Frederick-Stein/Data-Science-Playground/blob/main/CausalInference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRMzv-hhKuVN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## generate data\n",
        "def generate_data(\n",
        "    n=1000, seed=0, beta1=1.05, alpha1=0.4, alpha2=0.3,\n",
        "    binary_treatment=True, binary_cutoff=3.5, return_torch=False,\n",
        "    device=\"cpu\", dtype=torch.float32\n",
        "):\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    age = torch.normal(65.0, 5.0, (n,), device=device, dtype=dtype)\n",
        "    sodium = age / 18.0 + torch.randn(n, device=device, dtype=dtype)\n",
        "\n",
        "    if binary_treatment:\n",
        "        if binary_cutoff is None:\n",
        "            binary_cutoff = sodium.mean().item()   # python float\n",
        "        sodium = (sodium > binary_cutoff).to(dtype)  # 0/1 float\n",
        "\n",
        "    blood_pressure = beta1 * sodium + 2.0 * age + torch.randn(n, device=device, dtype=dtype)\n",
        "    proteinuria = alpha1 * sodium + alpha2 * blood_pressure + torch.randn(n, device=device, dtype=dtype)\n",
        "    hypertension = (blood_pressure >= 140).to(torch.int64)  # optional to return\n",
        "\n",
        "    if return_torch:\n",
        "        return {\n",
        "            \"blood_pressure\": blood_pressure,\n",
        "            \"sodium\": sodium,\n",
        "            \"age\": age,\n",
        "            \"proteinuria\": proteinuria,\n",
        "            \"hypertension\": hypertension,\n",
        "        }\n",
        "\n",
        "    to_np = lambda t: t.detach().cpu().numpy()\n",
        "    return pd.DataFrame({\n",
        "        \"blood_pressure\": to_np(blood_pressure),\n",
        "        \"sodium\": to_np(sodium),\n",
        "        \"age\": to_np(age),\n",
        "        \"proteinuria\": to_np(proteinuria),\n",
        "        \"hypertension\": to_np(hypertension),\n",
        "    })"
      ],
      "metadata": {
        "id": "qOzUQANNLx5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## prepare data\n",
        "data = generate_data(n=1000, seed=42, beta1=1.05, alpha1=0.4, alpha2=0.3, binary_treatment=True, binary_cutoff=3.5, return_torch=True)\n",
        "age = data['age'].float()\n",
        "sodium= data['sodium'].float()\n",
        "pro = data['proteinuria'].float()\n",
        "X = torch.stack([sodium, age, pro], dim = 1).float()\n",
        "# X = torch.stack([sodium, age], dim = 1).float()\n",
        "y = data['blood_pressure'].float().unsqueeze(1)\n",
        "\n",
        "train_data = TensorDataset(X, y)\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "AX1Jas_qN4YK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## construct model\n",
        "class LinearRegression(nn.Module):\n",
        "    def __init__(self, input_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_dim, out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)"
      ],
      "metadata": {
        "id": "fU0aGcWtTRbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## train model\n",
        "model = LinearRegression(3, 1)\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay = 1e-4)\n",
        "\n",
        "epochs = 50\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        y_pred = model(X_batch)\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # y_pred = model(X)\n",
        "    # loss = loss_fn(y_pred, y)\n",
        "\n",
        "    # optimizer.zero_grad()\n",
        "    # loss.backward()\n",
        "    # optimizer.step()"
      ],
      "metadata": {
        "id": "crqMHpp5MjZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## evaluate result\n",
        "with torch.inference_mode():\n",
        "    X1 = X.clone()\n",
        "    X1[:, 0] = 1.0\n",
        "    X0 = X.clone()\n",
        "    X0[:, 0] = 0.0\n",
        "    ATE_est = (model(X1) - model(X0)).mean().item()\n",
        "    print(f\"Estimated ATE: {ATE_est:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnZ74k79Ul1m",
        "outputId": "f8a0f82b-34e9-4de3-9616-16b955e4cd42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated ATE: 1.5077\n"
          ]
        }
      ]
    }
  ]
}
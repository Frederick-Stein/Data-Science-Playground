{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJDNEnLeL6aoSo0dFhd8ej",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Frederick-Stein/Data-Science-Playground/blob/main/Multi_head_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_fq8T6bn5jDR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadedSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim: int, num_heads: int):\n",
        "        super().__init__()\n",
        "\n",
        "        # torch.manual_seed(42)\n",
        "        assert embedding_dim % num_heads == 0 # divisible\n",
        "        self.head_dim = embedding_dim // num_heads\n",
        "\n",
        "        # Bulid head\n",
        "        self.att_heads = nn.ModuleList()\n",
        "        for i in range(num_heads):\n",
        "            self.att_heads.append(self.SingleHeadAttention(embedding_dim, self.head_dim))\n",
        "\n",
        "        # Output projection back to embedding_dim\n",
        "        self.W_output = nn.Linear(num_heads * self.head_dim, embedding_dim, bias=False)\n",
        "\n",
        "    def forward(self, embedded: torch.Tensor):\n",
        "        # embedded: (B, L, embedding_dim)\n",
        "\n",
        "        head_outputs = []\n",
        "        for head in self.att_heads:\n",
        "            head_outputs.append(head(embedded)) # (B, L, head_dim)\n",
        "        concatenated = torch.cat(head_outputs, dim = 2) # (B, L, num_heads * head_dim)\n",
        "        output = self.W_output(concatenated) # (B, L, embedding_dim)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "    class SingleHeadAttention(nn.Module):\n",
        "        def __init__(self, embedding_dim: int, head_dim: int):\n",
        "            super().__init__()\n",
        "\n",
        "            # torch.manual_seed(42)\n",
        "            self.head_dim = head_dim\n",
        "            self.W_q = nn.Linear(embedding_dim, head_dim, bias=False)\n",
        "            self.W_k = nn.Linear(embedding_dim, head_dim, bias=False)\n",
        "            self.W_v = nn.Linear(embedding_dim, head_dim, bias=False)\n",
        "\n",
        "        def forward(self, embedded: torch.Tensor):\n",
        "            # embedded: (B, L, embedding_dim)\n",
        "            B, L, _ = embedded.shape\n",
        "\n",
        "            Q = self.W_q(embedded) # (B, L, head_dim)\n",
        "            K = self.W_k(embedded)\n",
        "            V = self.W_v(embedded)\n",
        "\n",
        "            scores = Q @ K.transpose(1, 2)\n",
        "            scaled_scores = scores / (self.head_dim ** 0.5)\n",
        "\n",
        "            # Causal mask\n",
        "            mask = torch.triu(torch.ones(L, L, device=scores.device, dtype=torch.bool), diagonal=1)\n",
        "            scaled_scores = scaled_scores.masked_fill(mask.unsqueeze(0), float('-inf'))\n",
        "            attention_weights = F.softmax(scaled_scores, dim = 2) # (B, L, L)\n",
        "            attention_out = attention_weights @ V # (B, L, head_dim)\n",
        "\n",
        "            return attention_out"
      ],
      "metadata": {
        "id": "LT9UUnjTQxAZ"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}

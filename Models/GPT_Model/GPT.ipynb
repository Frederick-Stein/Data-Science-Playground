{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOTQ7dgmu84FGHl3ZtNcyv9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Frederick-Stein/Data-Science-Playground/blob/main/GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XF1BQNXdmoPC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## encode data\n",
        "def Encode(sentences: list):\n",
        "\n",
        "    words = set()\n",
        "    for sentence in sentences:\n",
        "        for word in sentence.split():\n",
        "            words.add(word)\n",
        "\n",
        "    token_to_id = {\"<PAD>\": 0, \"<EOS>\": 1}\n",
        "    id_to_token = {0: \"<PAD>\", 1: \"<EOS>\"}\n",
        "    for i, word in enumerate(words, start = 2):\n",
        "        token_to_id[word] = i\n",
        "        id_to_token[i] = word\n",
        "\n",
        "    vocab_size = len(token_to_id)\n",
        "\n",
        "    def encode(sentence):\n",
        "        ids = []\n",
        "        for word in sentence.split():\n",
        "            ids.append(token_to_id[word])\n",
        "        return ids\n",
        "\n",
        "    encoded_sentences = [torch.tensor(encode(sentence)) for sentence in sentences]\n",
        "    padded_sentences =  nn.utils.rnn.pad_sequence(encoded_sentences, batch_first=True, padding_value=0)\n",
        "    return vocab_size, padded_sentences, token_to_id, id_to_token"
      ],
      "metadata": {
        "id": "LClzf1hbnIef"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size: int, max_context_length: int, embedding_dim: int, dropout: float = 0.1, pad_idx: int | None = None):\n",
        "\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.position_embeddings = nn.Embedding(max_context_length, embedding_dim, padding_idx = pad_idx)\n",
        "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
        "\n",
        "        # Cache [0, 1, 2, ..., max_context_len-1] as a buffer\n",
        "        self.register_buffer(\n",
        "            \"position_ids\",\n",
        "            torch.arange(max_context_length, dtype=torch.long).unsqueeze(0),  # (1, max_len)\n",
        "            persistent=False\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, context: torch.Tensor):\n",
        "        # context (B, L) B : batch size, L: context length\n",
        "        assert context.dtype == torch.long\n",
        "        B, L = context.shape\n",
        "        word_embeddings = self.word_embeddings(context) * math.sqrt(self.embedding_dim)# (B, L, embedding_dim)\n",
        "        positions = self.position_ids[:, :L] # (1, L)\n",
        "        position_embeddings = self.position_embeddings(positions) # (1ï¼ŒL, embedding_dim)\n",
        "        output = word_embeddings + position_embeddings\n",
        "        return self.dropout(output)"
      ],
      "metadata": {
        "id": "yUsnGpyZntSy"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## test embedding\n",
        "B, L, d = 2, 5, 32\n",
        "vocab = 1000\n",
        "max_len = 16\n",
        "emb = Embedding(vocab, max_len, d, dropout=0.1, pad_idx=0)\n",
        "\n",
        "x = torch.randint(1, vocab, (B, L))       # non-pad tokens\n",
        "x[0, -1] = 0                              # add a pad token to test pad_idx\n",
        "y = emb(x)\n",
        "assert y.shape == (B, L, d)\n",
        "assert torch.isfinite(y).all()"
      ],
      "metadata": {
        "id": "VfIpnKyzvdNm"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadedSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim: int, num_heads: int, dropout: float = 0.1, causal: bool = True):\n",
        "        super().__init__()\n",
        "\n",
        "        assert embedding_dim % num_heads == 0 # divisible\n",
        "        self.head_dim = embedding_dim // num_heads\n",
        "\n",
        "        # Bulid head\n",
        "        self.att_heads = nn.ModuleList()\n",
        "        for i in range(num_heads):\n",
        "            self.att_heads.append(self.SingleHeadAttention(embedding_dim, self.head_dim, dropout, causal))\n",
        "\n",
        "        # Output projection back to embedding_dim\n",
        "        self.W_output = nn.Linear(num_heads * self.head_dim, embedding_dim, bias=False)\n",
        "        self.proj_dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, embedded: torch.Tensor, attn_mask: torch.Tensor | None = None):\n",
        "        # embedded: (B, L, embedding_dim)\n",
        "\n",
        "        head_outputs = []\n",
        "        for head in self.att_heads:\n",
        "            head_outputs.append(head(embedded, attn_mask = attn_mask)) # (B, L, head_dim)\n",
        "        concat_heads= torch.cat(head_outputs, dim = 2) # (B, L, num_heads * head_dim)\n",
        "        output = self.W_output(concat_heads) # (B, L, embedding_dim)\n",
        "\n",
        "        return self.proj_dropout(output)\n",
        "\n",
        "\n",
        "\n",
        "    class SingleHeadAttention(nn.Module):\n",
        "        def __init__(self, embedding_dim: int, head_dim: int, dropout: float = 0.1, causal: bool = True):\n",
        "            super().__init__()\n",
        "\n",
        "            self.causal = causal\n",
        "            self.head_dim = head_dim\n",
        "            self.W_q = nn.Linear(embedding_dim, head_dim, bias=False)\n",
        "            self.W_k = nn.Linear(embedding_dim, head_dim, bias=False)\n",
        "            self.W_v = nn.Linear(embedding_dim, head_dim, bias=False)\n",
        "            self.attn_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        def forward(self, embedded: torch.Tensor, attn_mask: torch.Tensor | None = None):\n",
        "            # embedded: (B, L, embedding_dim)\n",
        "            B, L, _ = embedded.shape\n",
        "\n",
        "            Q = self.W_q(embedded) # (B, L, head_dim)\n",
        "            K = self.W_k(embedded)\n",
        "            V = self.W_v(embedded)\n",
        "\n",
        "            scores = Q @ K.transpose(-2, -1)\n",
        "            scaled_scores = scores / (self.head_dim ** 0.5)\n",
        "\n",
        "            # Causal mask\n",
        "            if self.causal:\n",
        "                mask = torch.triu(torch.ones(L, L, device=scores.device, dtype=torch.bool), diagonal=1)\n",
        "                scaled_scores = scaled_scores.masked_fill(mask.unsqueeze(0), float('-inf'))\n",
        "\n",
        "            # padding mask: attn_mask expected shape (B, L) with True for keep/1 for tokens\n",
        "            if attn_mask is not None:\n",
        "                key_keep = attn_mask.to(torch.bool).unsqueeze(1) # (B, 1, L)\n",
        "                scaled_scores = scaled_scores.masked_fill(~key_keep, float('-inf'))\n",
        "\n",
        "            attention_weights = F.softmax(scaled_scores, dim = -1) # (B, L, L)\n",
        "            attention_weights = self.attn_dropout(attention_weights)\n",
        "            attention_out = attention_weights @ V # (B, L, head_dim)\n",
        "\n",
        "            return attention_out"
      ],
      "metadata": {
        "id": "c872pWLZm66F"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## test attention\n",
        "def test_mhsa_basic():\n",
        "    torch.manual_seed(0)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    B, L, d_model, heads = 2, 5, 32, 4\n",
        "    x = torch.randn(B, L, d_model, device=device)\n",
        "\n",
        "    mhsa = MultiHeadedSelfAttention(embedding_dim=d_model, num_heads=heads).to(device)\n",
        "    y = mhsa(x)\n",
        "\n",
        "    # 1) shape check\n",
        "    assert y.shape == (B, L, d_model), f\"bad shape: {y.shape}\"\n",
        "\n",
        "    # 2) no NaNs/Infs\n",
        "    assert torch.isfinite(y).all(), \"NaN/Inf in output\"\n",
        "\n",
        "    # 3) gradients flow\n",
        "    loss = y.pow(2).mean()\n",
        "    loss.backward()  # should populate grads\n",
        "    has_grad = any(p.grad is not None and torch.isfinite(p.grad).all() for p in mhsa.parameters())\n",
        "    assert has_grad, \"no finite gradients\"\n",
        "\n",
        "    print(\"âœ“ basic: shape, finiteness, gradients\")\n",
        "\n",
        "def test_mhsa_causality():\n",
        "    \"\"\"\n",
        "    If we change *future* tokens, earlier outputs must remain unchanged.\n",
        "    \"\"\"\n",
        "    torch.manual_seed(1)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    B, L, d_model, heads = 1, 6, 24, 3\n",
        "    x = torch.zeros(B, L, d_model, device=device)\n",
        "\n",
        "    # Put a random signal ONLY in the last position\n",
        "    x[:, -1, :] = torch.randn(d_model, device=device)\n",
        "\n",
        "    mhsa = MultiHeadedSelfAttention(embedding_dim=d_model, num_heads=heads).to(device)\n",
        "    y1 = mhsa(x).detach()\n",
        "\n",
        "    # Now change the *future* token (last position) and re-run\n",
        "    x2 = x.clone()\n",
        "    x2[:, -1, :] = torch.randn(d_model, device=device)  # different future content\n",
        "    y2 = mhsa(x2).detach()\n",
        "\n",
        "    # Earlier positions [0 .. L-2] should be identical; last position can change\n",
        "    diff_early = (y1[:, :L-1, :] - y2[:, :L-1, :]).abs().max().item()\n",
        "    diff_last  = (y1[:, L-1:, :]   - y2[:, L-1:, :]).abs().max().item()\n",
        "\n",
        "    assert diff_early < 1e-6, f\"causality broken: early diff={diff_early:.3e}\"\n",
        "    assert diff_last >= 1e-6, \"last position should change when its input changes\"\n",
        "\n",
        "    print(f\"âœ“ causality: early max diff={diff_early:.3e}, last changed\")\n",
        "\n",
        "def test_mhsa_divisibility_guard():\n",
        "    try:\n",
        "        _ = MultiHeadedSelfAttention(embedding_dim=30, num_heads=8)  # not divisible\n",
        "        raise AssertionError(\"expected an assertion for non-divisible dims\")\n",
        "    except AssertionError:\n",
        "        print(\"âœ“ divisibility check triggers\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_mhsa_basic()\n",
        "    test_mhsa_causality()\n",
        "    test_mhsa_divisibility_guard()\n",
        "    print(\"All tests passed âœ”\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPKGl0CWEAck",
        "outputId": "b0d3d9a1-bd51-4ac7-f0ec-e6f669b579e9"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ basic: shape, finiteness, gradients\n",
            "âœ“ causality: early max diff=0.000e+00, last changed\n",
            "âœ“ divisibility check triggers\n",
            "All tests passed âœ”\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FFN(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim: int, dropout: float = 0.1, expansion_factor: int = 4):\n",
        "        super().__init__()\n",
        "        hidden_dim = expansion_factor * embedding_dim\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, hidden_dim), # up-projection\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, embedding_dim), # down-projection\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.block(x)"
      ],
      "metadata": {
        "id": "laEmXR392xFT"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim: int, num_heads: int, dropout: float = 0.1, causal: bool = True):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadedSelfAttention(embedding_dim, num_heads, dropout=dropout, causal=causal)\n",
        "        self.ffn = FFN(embedding_dim, dropout=dropout)\n",
        "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
        "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
        "        # optional residual dropouts\n",
        "        self.attn_dropout = nn.Dropout(dropout)\n",
        "        self.ffn_dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor, attn_mask: torch.Tensor | None = None):\n",
        "        # embedded: (B, L, embedding_dim)\n",
        "        x = x + self.attn_dropout(self.attention(self.norm1(x), attn_mask = attn_mask))\n",
        "        x = x + self.ffn_dropout(self.ffn(self.norm2(x)))\n",
        "        return x"
      ],
      "metadata": {
        "id": "HwwACIqt7ME9"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## test TransformerBlock\n",
        "def make_pad_batch(lengths, d_model, pad_id=0):\n",
        "    \"\"\"Create a (B, L, d) batch with padding mask (True=token, False=pad).\"\"\"\n",
        "    B = len(lengths)\n",
        "    L = max(lengths)\n",
        "    x = torch.randn(B, L, d_model)\n",
        "    mask = torch.zeros(B, L, dtype=torch.bool)\n",
        "    for i, t in enumerate(lengths):\n",
        "        mask[i, :t] = True\n",
        "        if t < L:\n",
        "            x[i, t:, :] = 0.0  # content of pads doesn't matter\n",
        "    return x, mask  # (B,L,d), (B,L)\n",
        "\n",
        "# ====== tests ======\n",
        "def test_basic_forward():\n",
        "    torch.manual_seed(0)\n",
        "    B, L, d, h = 2, 8, 64, 4\n",
        "    x = torch.randn(B, L, d)\n",
        "    blk = TransformerBlock(d, h, dropout=0.1, causal=False)\n",
        "    y = blk(x, attn_mask=torch.ones(B, L, dtype=torch.bool))\n",
        "    assert y.shape == (B, L, d), f\"bad shape {y.shape}\"\n",
        "    assert torch.isfinite(y).all(), \"NaN/Inf in output\"\n",
        "    print(\"âœ“ basic forward: shape & finiteness\")\n",
        "\n",
        "def test_backward_grad():\n",
        "    torch.manual_seed(1)\n",
        "    B, L, d, h = 2, 6, 32, 4\n",
        "    x = torch.randn(B, L, d, requires_grad=True)\n",
        "    blk = TransformerBlock(d, h, dropout=0.1, causal=False)\n",
        "    y = blk(x, attn_mask=torch.ones(B, L, dtype=torch.bool))\n",
        "    loss = y.pow(2).mean()\n",
        "    loss.backward()\n",
        "    # at least some params should have grad\n",
        "    has_grad = any(p.grad is not None and torch.isfinite(p.grad).all() for p in blk.parameters())\n",
        "    assert has_grad, \"no finite gradients\"\n",
        "    print(\"âœ“ backward: gradients flow\")\n",
        "\n",
        "def test_padding_mask_blocks_pads():\n",
        "    torch.manual_seed(2)\n",
        "    d, h = 48, 3\n",
        "    lengths = [5, 3]    # batch with different sequence lengths\n",
        "    x, mask = make_pad_batch(lengths, d_model=d)  # mask: True=token, False=pad\n",
        "\n",
        "    blk = TransformerBlock(d, h, dropout=0.0, causal=False)  # turn off dropout for determinism\n",
        "    blk.eval()\n",
        "\n",
        "    # Run once with correct mask\n",
        "    y_masked = blk(x.clone(), attn_mask=mask)\n",
        "\n",
        "    # Run again pretending pads are real tokens (all True)\n",
        "    y_unmasked = blk(x.clone(), attn_mask=torch.ones_like(mask))\n",
        "\n",
        "    # For padded positions, outputs SHOULD differ (since unmasked lets pads influence others)\n",
        "    # For real token positions, keep the outputs close (they might also differ slightly)\n",
        "    B, L = mask.shape\n",
        "    pad_positions = ~mask\n",
        "    if pad_positions.any():\n",
        "        diff_on_pads = (y_masked[pad_positions] - y_unmasked[pad_positions]).abs().mean().item()\n",
        "        assert diff_on_pads > 1e-6, \"padding mask seems ineffective\"\n",
        "    print(\"âœ“ padding mask: pads are blocked\")\n",
        "\n",
        "def test_causality():\n",
        "    torch.manual_seed(3)\n",
        "    B, L, d, h = 1, 6, 32, 4\n",
        "    x = torch.zeros(B, L, d)\n",
        "    # put signal only at the last position\n",
        "    x[:, -1, :] = torch.randn(d)\n",
        "\n",
        "    blk = TransformerBlock(d, h, dropout=0.0, causal=True)  # causal on\n",
        "    blk.eval()\n",
        "\n",
        "    y1 = blk(x.clone(), attn_mask=torch.ones(B, L, dtype=torch.bool)).detach()\n",
        "    # change only the last (future) token\n",
        "    x[:, -1, :] = torch.randn(d)\n",
        "    y2 = blk(x.clone(), attn_mask=torch.ones(B, L, dtype=torch.bool)).detach()\n",
        "\n",
        "    # Earlier positions must be identical; last can change\n",
        "    early_diff = (y1[:, :L-1, :] - y2[:, :L-1, :]).abs().max().item()\n",
        "    last_diff  = (y1[:, L-1:, :]   - y2[:, L-1:, :]).abs().max().item()\n",
        "    assert early_diff < 1e-6, f\"causality broken: early_diff={early_diff:.3e}\"\n",
        "    assert last_diff >= 1e-6, \"last position should change\"\n",
        "    print(\"âœ“ causality: future tokens don't affect the past\")\n",
        "\n",
        "def test_train_vs_eval_dropout():\n",
        "    torch.manual_seed(4)\n",
        "    B, L, d, h = 2, 8, 64, 4\n",
        "    x = torch.randn(B, L, d)\n",
        "    mask = torch.ones(B, L, dtype=torch.bool)\n",
        "    blk = TransformerBlock(d, h, dropout=0.2, causal=False)\n",
        "\n",
        "    blk.train()\n",
        "    y_train1 = blk(x, attn_mask=mask)\n",
        "    y_train2 = blk(x, attn_mask=mask)\n",
        "    # in train mode with dropout, two passes usually differ\n",
        "    train_diff = (y_train1 - y_train2).abs().mean().item()\n",
        "    assert train_diff > 0.0, \"dropout not active in train mode?\"\n",
        "\n",
        "    blk.eval()\n",
        "    y_eval1 = blk(x, attn_mask=mask)\n",
        "    y_eval2 = blk(x, attn_mask=mask)\n",
        "    eval_diff = (y_eval1 - y_eval2).abs().max().item()\n",
        "    assert eval_diff < 1e-7, \"eval should be deterministic (no dropout)\"\n",
        "    print(\"âœ“ dropout behavior: stochastic in train, deterministic in eval\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_basic_forward()\n",
        "    test_backward_grad()\n",
        "    test_padding_mask_blocks_pads()\n",
        "    test_causality()\n",
        "    test_train_vs_eval_dropout()\n",
        "    print(\"All tests passed âœ”\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7qX3GB2JWLW",
        "outputId": "be0bfaa4-4a1b-4164-f7da-d08cb11f8517"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ basic forward: shape & finiteness\n",
            "âœ“ backward: gradients flow\n",
            "âœ“ padding mask: pads are blocked\n",
            "âœ“ causality: future tokens don't affect the past\n",
            "âœ“ dropout behavior: stochastic in train, deterministic in eval\n",
            "All tests passed âœ”\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size: int, embedding_dim: int, max_context_length: int, num_blocks: int, num_heads: int, dropout: float = 0.1, causal: bool = True):\n",
        "        super().__init__()\n",
        "        self.embedding = Embedding(vocab_size, max_context_length, embedding_dim, dropout=dropout, pad_idx=0)\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(embedding_dim, num_heads, dropout=dropout, causal=causal) for _ in range(num_blocks)\n",
        "        ])\n",
        "        self.final_norm = nn.LayerNorm(embedding_dim)\n",
        "        self.vocab_projection = nn.Linear(embedding_dim, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, context: torch.Tensor, attn_mask: torch.Tensor | None = None):\n",
        "        # context: (B, L)\n",
        "        # attn_mask (B, L)\n",
        "        x = self.embedding(context) # (B, L, d)\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, attn_mask = attn_mask) # (B, L, d)\n",
        "        x = self.final_norm(x) # (B, L, d)\n",
        "        raw_output = self.vocab_projection(x) # logits\n",
        "\n",
        "        return raw_output"
      ],
      "metadata": {
        "id": "AL_9URnTvzyL"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## test gpt\n",
        "\n",
        "def test_forward_shapes():\n",
        "    torch.manual_seed(0)\n",
        "    vocab_size, d_model, max_len = 100, 32, 16\n",
        "    num_blocks, num_heads = 2, 4\n",
        "    model = GPT(vocab_size, d_model, max_len, num_blocks, num_heads, dropout=0.1, causal=True)\n",
        "\n",
        "    B, L = 3, 10\n",
        "    x = torch.randint(0, vocab_size, (B, L))      # fake token ids\n",
        "    mask = torch.ones(B, L, dtype=torch.bool)     # no padding\n",
        "    logits = model(x, attn_mask=mask)\n",
        "\n",
        "    assert logits.shape == (B, L, vocab_size)\n",
        "    print(\"âœ“ forward: logits shape OK\", logits.shape)\n",
        "\n",
        "def test_backward_grad():\n",
        "    torch.manual_seed(1)\n",
        "    vocab_size, d_model, max_len = 50, 32, 12\n",
        "    model = GPT(vocab_size, d_model, max_len, num_blocks=2, num_heads=4, dropout=0.1, causal=True)\n",
        "\n",
        "    B, L = 2, 8\n",
        "    x = torch.randint(0, vocab_size, (B, L))\n",
        "    mask = torch.ones(B, L, dtype=torch.bool)\n",
        "    logits = model(x, attn_mask=mask)\n",
        "\n",
        "    # dummy LM loss: predict next token\n",
        "    loss = F.cross_entropy(\n",
        "        logits[:, :-1, :].reshape(-1, vocab_size),\n",
        "        x[:, 1:].reshape(-1),\n",
        "    )\n",
        "    loss.backward()\n",
        "\n",
        "    has_grad = any(p.grad is not None and torch.isfinite(p.grad).all() for p in model.parameters())\n",
        "    assert has_grad, \"No finite gradients!\"\n",
        "    print(\"âœ“ backward: loss computed & gradients flow\")\n",
        "\n",
        "def test_causality():\n",
        "    torch.manual_seed(2)\n",
        "    vocab_size, d_model, max_len = 40, 16, 10\n",
        "    model = GPT(vocab_size, d_model, max_len, num_blocks=1, num_heads=2, dropout=0.0, causal=True)\n",
        "    model.eval()\n",
        "\n",
        "    B, L = 1, 6\n",
        "    x = torch.randint(0, vocab_size, (B, L))\n",
        "\n",
        "    # Run once\n",
        "    y1 = model(x).detach()\n",
        "    # Change only the last token (future)\n",
        "    x2 = x.clone()\n",
        "    x2[:, -1] = (x2[:, -1] + 1) % vocab_size\n",
        "    y2 = model(x2).detach()\n",
        "\n",
        "    # Earlier logits [0..L-2] should match exactly if causal is working\n",
        "    diff_early = (y1[:, :-1, :] - y2[:, :-1, :]).abs().max().item()\n",
        "    print(\"âœ“ causality check: max diff before last =\", diff_early)\n",
        "\n",
        "def test_dropout_behavior():\n",
        "    torch.manual_seed(3)\n",
        "    vocab_size, d_model, max_len = 30, 16, 12\n",
        "    model = GPT(vocab_size, d_model, max_len, num_blocks=1, num_heads=2, dropout=0.2, causal=True)\n",
        "\n",
        "    B, L = 2, 5\n",
        "    x = torch.randint(0, vocab_size, (B, L))\n",
        "    mask = torch.ones(B, L, dtype=torch.bool)\n",
        "\n",
        "    model.train()\n",
        "    y1 = model(x, attn_mask=mask)\n",
        "    y2 = model(x, attn_mask=mask)\n",
        "    diff_train = (y1 - y2).abs().mean().item()\n",
        "\n",
        "    model.eval()\n",
        "    y3 = model(x, attn_mask=mask)\n",
        "    y4 = model(x, attn_mask=mask)\n",
        "    diff_eval = (y3 - y4).abs().max().item()\n",
        "\n",
        "    print(f\"âœ“ dropout train-mode diff ~ {diff_train:.4f}, eval-mode diff ~ {diff_eval:.4e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_forward_shapes()\n",
        "    test_backward_grad()\n",
        "    test_causality()\n",
        "    test_dropout_behavior()\n",
        "    print(\"All GPT tests passed âœ”\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AeqEvTjOOFP",
        "outputId": "36539a7f-1cfb-4485-a465-b1f753fd334b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ forward: logits shape OK torch.Size([3, 10, 100])\n",
            "âœ“ backward: loss computed & gradients flow\n",
            "âœ“ causality check: max diff before last = 0.0\n",
            "âœ“ dropout train-mode diff ~ 0.3606, eval-mode diff ~ 0.0000e+00\n",
            "All GPT tests passed âœ”\n"
          ]
        }
      ]
    }
  ]
}
